ğŸš€ BNN Hardware Accelerator



SystemVerilog-based FPGA/ASIC Accelerator for Binary Neural Networks



A high-performance Binary Neural Network (BNN) accelerator implemented in SystemVerilog, optimized for FPGA and ASIC deployment.

The design achieves 79.29% test accuracy on CIFAR-10 using efficient XNORâ€“Popcount compute primitives and sparsity-aware optimizations.



ğŸ“Œ Overview



This project implements a fully hardware-mapped BNN inference engine featuring:



Bitwise XNORâ€“Popcount MAC replacement



Broadcast and Systolic dataflow architectures



Sparse-aware Processing Elements (PEs)



Clock-gated low-power design



Compressed tiled weight storage (.hex-based loading)



Full verification testbench



The accelerator is designed for energy-efficient edge AI inference.



ğŸ§  Design Architecture

1ï¸âƒ£ Compute Primitive



Binary convolution using XNOR + Popcount



Replaces traditional multiply-accumulate (MAC)



Significantly reduces DSP usage



2ï¸âƒ£ Dataflow Architectures

ğŸ”¹ Broadcast Topology



Centralized weight broadcasting



Lower control complexity



Suitable for smaller PE arrays



ğŸ”¹ Systolic Array



Pipelined data propagation



High throughput



Better scalability for large feature maps



3ï¸âƒ£ Processing Elements

âš™ï¸ adaptive\_pe.sv



Runtime performance monitoring



Dynamic utilization tracking



Configurable compute pipeline depth



âš™ï¸ sparse\_aware\_pe.sv



Zero-skipping logic



Sparsity exploitation



Reduced switching activity



4ï¸âƒ£ Memory System



Tiled weight storage



Compressed binary weights



.hex-based initialization



FPGA BRAM friendly



ğŸ“‚ Repository Structure

File	Description

accelerator\_top\_broadcast.sv	Broadcast-style top-level architecture

accelerator\_top\_systolic.sv	Systolic array top-level architecture

adaptive\_pe.sv	Adaptive performance-aware PE

sparse\_aware\_pe.sv	Sparsity-exploiting PE

tb\_compressed\_accelerator.sv	Full-system verification testbench

ğŸ›  Toolchain

Stage	Tool

RTL Design	SystemVerilog

Simulation	Vivado / ModelSim

Training	PyTorch

Dataset	CIFAR-10

Target	FPGA / ASIC

ğŸ“Š Performance Results



Test Accuracy: 79.29% on CIFAR-10



Binary convolution via XNOR-popcount



Supports sparse weight compression



Clock gating enabled for dynamic power reduction



FPGA-friendly BRAM mapping



ğŸ”‹ Power Optimization Features



Clock gating in inactive PEs



Sparse-aware zero skipping



Reduced switching activity through binary arithmetic



DSP-free computation



ğŸ§ª Verification



Full SystemVerilog testbench (tb\_compressed\_accelerator.sv)



Compressed weight loading validation



End-to-end inference verification



Functional correctness against PyTorch baseline



ğŸ¯ Target Applications



Edge AI inference



Low-power embedded vision



FPGA-based AI accelerators



Custom ASIC ML inference engines



ğŸ“ˆ Key Contributions



Dual-topology BNN hardware implementation



Adaptive PE architecture with monitoring logic



Sparse-aware execution model



Hardwareâ€“software co-design workflow (PyTorch â†’ .hex â†’ RTL)

